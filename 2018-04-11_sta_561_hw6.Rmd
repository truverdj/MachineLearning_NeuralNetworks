---
title: "STA 561 HW6 (I never want to see Neural Networks again)"
author: "Daniel Truver"
date: "3/26/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(dplyr)
library(ggplot2)
theme_set(theme_bw())
```

#### (1.1) Neural Networks and Universal Approximation Theorem

Full disclosure: I don't like Neural Networks, I don't want to use them, I don't understand them, and there's not much about them in the lecture notes. That said, please enjoy the following train fire of a homework problem.

##### (a) What and why?

As far as I can tell from the literature, all neural networks are a lot of guesswork followed by justification with no real indication of the creation process for the NN. So, here is my approximation to the bump which I just thought up in a very methodical manner and arrived at directly with no intermediary steps or cursing.

```{r}
sigmoid = function(x){
  1/(1 + exp(-x))
}
bump1 = function(x){
  bump = 0.4 <= x & x <= 0.6
  y = x
  y[bump] = 1
  y[!bump] = 0
  y
}
NN = function(x){
  w = 256
  neuron1 = sigmoid(w*(x-0.4))
  neuron2 = sigmoid(w*(x-0.6))
  o = neuron1 - neuron2
  o
}
x1 = seq(0,1,length.out = 500)
NN1 = NN(x1)
truebump = bump1(x1)
ggplot(data = data.frame(x = x1, NN = NN1, truth = truebump), aes(x = x)) +
  geom_line(aes(y = truth, color = "truth")) +
  geom_line(aes(y = NN, color = "NN")) +
  ggtitle("Looks Good...I think?")
```

Wow! And it has exactly half the neurons of my brain. See attached drawing 1.

##### (b) Parameters

(1) The weights determine the steepness of the bump. (2) The bias determines the location. (3) The height is just the height of the activation function, the sigmoid; we would need a multiple of the sigmoid to change the height.

#### (1.2) The suffering continues

##### (a) Behold! Another neural network.

```{r}
NN2 = function(x_1, x_2){
  w = 256
  neuron1 = sigmoid(w*(x_1-0.3))
  neuron2 = sigmoid(w*(x_1-0.7))
  o = neuron1 - neuron2
  o
}
x1 = seq(0,1, length.out = 100)
x2 = seq(0,1, length.out = 100)
y = outer(x1, x2, NN2)
lattice::wireframe(y, drape = TRUE, xlab = "x1", ylab = "x2")
```

See attached drawing 2. 

##### (b) Tower

```{r}
NN3 = function(x_1, x_2){
  w = 256
  neuron1 = sigmoid(w*(x_1-0.4))
  neuron2 = sigmoid(w*(x_1-0.6))
  o_1 = neuron1 - neuron2
  neuron3 = sigmoid(w*(x_2-0.4))
  neuron4 = sigmoid(w*(x_2-0.6))
  o_2 = neuron3 - neuron4
  o = 2*sigmoid(w*(o_1 + o_2 - 2))
  o
}
x1 = seq(0,1, length.out = 100)
x2 = seq(0,1, length.out = 100)
y = outer(x1, x2, NN3)
lattice::wireframe(y, drape = TRUE, xlab = "x1", ylab = "x2")
```

See attached drawing 3.

##### (c) Tower function approximation 

If we consider error measured by the $L_1$ metric and agree that the gradient exists in both directions at every point, then this approximation is familiar. It is the approximation of the Riemann integral for functions of 2 variables. The bases of the towers form the partition of the unit square. Let $(x_i,y_i)$ denote the center of the tower $i$ and let the height of each tower be $f(x_i,y_i)$. Additionally, denote the region covered by each tower's base to be $A_i$ and the area of the base to be $\Delta A_i$. Then, the error for each tower is
$$
\begin{aligned}
\int_{A_i}|f(x,y) - f(x_i,y_i)|~dxdy 
&\leq \int_{A_i} \left| \frac{t}{n} \right|~dxdy \\
&= \frac{|t|}{n}\Delta A_i \\
&= \frac{|t|}{n}\frac{1}{n^2} \\
&= \frac{|t|}{n^3}\\
\text{and we want} ~ \frac{|t|}{n^3} &< \epsilon
\end{aligned}
$$
where the first inequality is due to the known maximum change in the function and the length of the interval over which that change occurs ($n$ tower centers evenly spaced give each a square base of side length $1/n$).

So, by taking 
$$
n > \left(\frac{|t|}{\epsilon}\right)^{1/3}.
$$

So, when we take $n^2$ towers, and want to bound the total error $\delta$, then we need to bound
$$
n^2 \frac{|t|}{n^3} < \delta,
$$
which we can achieve with
$$
n > \frac{|t|}{\delta}.
$$
As $t$ increases, we will need more and more tower functions, therefore more and more neurons, to achieve our desired bound.