---
title: "STA 561 HW6 (I never want to see Neural Networks again)"
author: "Daniel Truver"
date: "3/26/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(dplyr)
library(ggplot2)
theme_set(theme_bw())
```

#### (1.1) Neural Networks and Universal Approximation Theorem

Full disclosure: I don't like Neural Networks, I don't want to use them, I don't understand them, and there's not much about them in the lecture notes. That said, please enjoy the following train fire of a homework problem.

##### (a) What and why?

As far as I can tell from the literature, all neural networks are a lot of guesswork followed by justification with no real indication of the creation process for the NN. So, here is my approximation to the bump which I just thought up in a very methodical manner and arrived at directly with no intermediary steps or cursing.

```{r}
sigmoid = function(x){
  1/(1 + exp(-x))
}
bump1 = function(x){
  bump = 0.4 <= x & x <= 0.6
  y = x
  y[bump] = 1
  y[!bump] = 0
  y
}
NN = function(x){
  w = 256
  neuron1 = sigmoid(w*(x-0.4))
  neuron2 = sigmoid(w*(x-0.6))
  o = neuron1 - neuron2
  o
}
x1 = seq(0,1,length.out = 500)
NN1 = NN(x1)
truebump = bump1(x1)
ggplot(data = data.frame(x = x1, NN = NN1, truth = truebump), aes(x = x)) +
  geom_line(aes(y = truth, color = "truth")) +
  geom_line(aes(y = NN, color = "NN")) +
  ggtitle("Looks Good...I think?")
```

Wow! And it has exactly half the neurons of my brain. See attached drawing 1.

##### (b) Parameters

(1) The weights determine the steepness of the bump. (2) The bias determines the location. (3) The height is just the height of the activation function, the sigmoid; we would need a multiple of the sigmoid to change the height.

#### (1.2) The suffering continues

##### (a) Behold! Another neural network.

```{r}
NN2 = function(x_1, x_2){
  w = 256
  neuron1 = sigmoid(w*(x_1-0.3))
  neuron2 = sigmoid(w*(x_1-0.7))
  o = neuron1 - neuron2
  o
}
x1 = seq(0,1, length.out = 100)
x2 = seq(0,1, length.out = 100)
y = outer(x1, x2, NN2)
lattice::wireframe(y, drape = TRUE, xlab = "x1", ylab = "x2")
```

See attached drawing 2. 

##### (b) Tower

```{r}
NN3 = function(x_1, x_2){
  w = 256
  neuron1 = sigmoid(w*(x_1-0.4))
  neuron2 = sigmoid(w*(x_1-0.6))
  o_1 = neuron1 - neuron2
  neuron3 = sigmoid(w*(x_2-0.4))
  neuron4 = sigmoid(w*(x_2-0.6))
  o_2 = neuron3 - neuron4
  o = 2*sigmoid(w*(o_1 + o_2 - 2))
  o
}
x1 = seq(0,1, length.out = 100)
x2 = seq(0,1, length.out = 100)
y = outer(x1, x2, NN3)
lattice::wireframe(y, drape = TRUE, xlab = "x1", ylab = "x2")
```

See attached drawing 3.

##### (c) Tower function approximation 

If we consider error measured by the $L_1$ metric and agree that the gradient exists in both directions at every point, then this approximation is familiar. It is the approximation of the Riemann integral for functions of 2 variables. The bases of the towers form the partition of the unit square. Let $(x_i,y_i)$ denote the center of the tower $i$ and let the height of each tower be $f(x_i,y_i)$. Additionally, denote the region covered by each tower's base to be $A_i$ and the area of the base to be $\Delta A_i$. Then, the error for each tower is
$$
\begin{aligned}
\int_{A_i}|f(x,y) - f(x_i,y_i)|~dxdy 
&\leq \int_{A_i} \left| \frac{t}{n} \right|~dxdy \\
&= \frac{|t|}{n}\Delta A_i \\
&= \frac{|t|}{n}\frac{1}{n^2} \\
&= \frac{|t|}{n^3}\\
\text{and we want} ~ \frac{|t|}{n^3} &< \epsilon
\end{aligned}
$$
where the first inequality is due to the known maximum change in the function and the length of the interval over which that change occurs ($n$ tower centers evenly spaced give each a square base of side length $1/n$).

So, by taking 
$$
n > \left(\frac{|t|}{\epsilon}\right)^{1/3}.
$$

So, when we take $n^2$ towers, and want to bound the total error $\delta$, then we need to bound
$$
n^2 \frac{|t|}{n^3} < \delta,
$$
which we can achieve with
$$
n > \frac{|t|}{\delta}.
$$
As $t$ increases, we will need more and more tower functions, therefore more and more neurons, to achieve our desired bound.

##### (2) EM (Laundry?)

Machine one is broken $\theta_1$ proportion of the time (that you try to use it?) and machine 2 is broken $\theta_2$ proportion of the time (that you try to use it?). So, it's Bernoulli whether the machine take your money and starts or takes your money and does not start. It's also Bernoulli (0.50) which machine you have to try when you get to the laundry, but the facilities fee on your tuition is always the same. At this point, instead of visiting the laundry $m$ times and having to risk my money $n$ times, I'd just go to Duke pond with some dish soap and give it my best. You'd think by this point someone from the engineering department would have made themselves useful and fixed the damn machines. Also, if we didn't write down which machine we used each time, we deserve to be dirty. 

##### (a) Using EM algorithm 

$m$ = number of days we vistited the laundry.
$n$ = number of laundry attempts in a single day.
$\theta_1$ = proportion of failed attempts on machine 1.
$\theta_2$ = proportion of failed attempts on machine 2. 
Which machine we choose is Bernoulli(0.50).
Number of failures on machine 1 in a single day is Binomial($n,\theta_1$).
Number of failures on machine 2 in a single day is Binomial($n,\theta_1$).
Let $X_1,\dots, X_m \in \{1,\dots,n\}$ be the number of failed attempts on each day. 
Let $Z_1,\dots, Z_m \in \{1,2\}$ be the machine used on each day (unobserved).
Let $\theta = \{\theta_1, \theta_2\}$.

First note that
$$
\begin{aligned}
P(X_i = x_i, Z_i = k\mid \theta) 
&= P(X_i = x_i\mid Z_i = k, \theta)P(Z_i = k\mid \theta) \\
&= \text{binom}(x_i;n, \theta_k)(0.50)
\end{aligned}
$$
where we obtain the last equality by noting that which machine we used on day $i$ was completely random.

Then our auxillary function is, at time $t$,
$$
\begin{aligned}
A(\theta, \theta^{(t)}) 
&= \sum_i\sum_k P(z_i = k\mid x_i, \theta^{(t)}) \log\frac{P(X_i = x_i, Z_i = k\mid\theta)}{P(z_i = k\mid x_i, \theta^{(t)})}\\
&= \sum_i\sum_k \frac{\text{binom}(x_i;n, \theta_k^{(t)})(0.50)}{\text{binom}(x_i;n, \theta_1^{(t)})(0.50) + \text{binom}(x_i;n, \theta_2^{(t)})(0.50)} \log\frac{\text{binom}(x_i;n, \theta_k)(0.50)}{\frac{\text{binom}(x_i;n, \theta_k^{(t)})(0.50)}{\text{binom}(x_i;n, \theta_1^{(t)})(0.50) + \text{binom}(x_i;n, \theta_2^{(t)})(0.50)}}\\
\end{aligned}
$$

For simplicity, let 
$$
\frac{\text{binom}(x_i;n, \theta_k^{(t)})}{\text{binom}(x_i;n, \theta_1^{(t)}) + \text{binom}(x_i;n, \theta_2^{(t)})} 
= \gamma_{ik}
$$
then 
$$
A(\theta, \theta^{(t)}) = \sum_i\sum_k \gamma_{ik} \log\frac{\text{binom}(x_i;n, \theta_k)(0.50)}{\gamma_{ik}}.
$$

For our next step, set 
$$
\theta^{(t+1)} = \arg\max_\theta A(\theta, \theta^{(t)}).
$$
##### (b) A Test (simulated data)

```{r, cache=TRUE}
theta = list("1" = 0.8, "2" = 0.3)
m = 6 
n = 100
failedAttempts = rep(NA, 6)
set.seed(2018)
for (j in 1:m){
  machine = sample(c(1,2), size = 1)
  p = theta[[machine]]
  failedAttempts[j] = rbinom(1, n, p)
}
failedAttempts
theta.t = list()
theta.t[[1]] = c(mean(failedAttempts[1:3]/100), mean(failedAttempts[4:6]/100))
for (t in 1:50){
  gamma_ik =  c(dbinom(failedAttempts, n, theta.t[[t]][1]), 
                dbinom(failedAttempts, n, theta.t[[t]][2]))/
    rep(dbinom(failedAttempts, n, theta.t[[t]][1]) + dbinom(failedAttempts, n, theta.t[[t]][2]), 2)
  A = function(theta_1, theta_2){
    sum(gamma_ik * c(log(0.5*dbinom(failedAttempts,n,theta_1)), 
                     log(0.5*dbinom(failedAttempts,n,theta_2))
                     )
    )
  }
  gridpts = 500
  x1 = seq(0,1,length.out = gridpts)
  x2 = seq(0,1,length.out = gridpts)
  gridMat = matrix(NA, ncol = gridpts, nrow = gridpts)
  for (i in seq_along(x1)) {
    for(j in seq_along(x2)){
      gridMat[i,j] = A(x1[i],x2[j])
    }
  }
  nextTheta = which(gridMat == max(gridMat), arr.ind = TRUE)
  theta.t[[t+1]] = c(x1[nextTheta[1]], x2[nextTheta[2]])
}
```

Well, the algorithm pretty much converged in one go because we chose our starting points by inspection of the data. We saw there were two groups and chose the starting points to be the means of the two groups.

```{r}
df = t(as.data.frame(theta.t))
rownames(df) = NULL; colnames(df) = c("theta_1", "theta_2")
ggplot(data.frame(df), aes(x = theta_1, y = theta_2)) +
  geom_jitter(width = .02, height = .02) +
  xlim(0,1) + ylim(0,1)
```

The points are jittered just to show there are multiple ones clustered together.

So, yeah...always pick your starting points in an intelligent way?

#### (3) Clustering

```{r}
df = read.csv("data.csv")
colnames(df) = c("x", "y")
```

##### (a) K-means

```{r}
euclid = function(x1, y1, x2, y2){
  sqrt((x2-x1)^2 + (y2-y1)^2)
}
k.means = function(data, clusters = 2, epsilon = 0.01){ # data should live in unit square
  startingPts = lapply(1:clusters, function(k){
    runif(2)
  })
  TSPmatrix = matrix(NA, nrow = nrow(data), ncol = clusters)
  for (i in 1:length(startingPts)){
    TSPmatrix[,i] = euclid(data$x, data$y, startingPts[[i]][1], startingPts[[i]][2])
  }
  assignment = apply(TSPmatrix, 1, which.min)
  newdata = cbind(data, assignment)
  convergence = 1
  while(convergence > epsilon){
    which.clusters = sort(unique(newdata$assignment))
    newCenters = lapply(which.clusters, function(k){
      mean_x = newdata %>%
        filter(assignment == k) %>%
        .$x %>%
        mean()
      mean_y = newdata %>%
        filter(assignment == k) %>%
        .$y %>%
        mean()
      c(mean_x, mean_y)
    })
    for (i in 1:length(newCenters)){
      TSPmatrix[,i] = euclid(newdata$x, newdata$y, newCenters[[i]][1], newCenters[[i]][2])
    }
    assignment = apply(TSPmatrix, 1, which.min)
    newdata = cbind(data, assignment)
    moves = rep(NA, length(newCenters))
    for (j in 1:length(newCenters)){
      moves[j] = euclid(startingPts[[j]][1], startingPts[[j]][2],
                        newCenters[[j]][1], newCenters[[j]][2])
    }
    convergence = max(moves)
    startingPts = newCenters
  }
  mean.ks = list()
  for (k in 1:length(unique(newdata$assignment))){
    mean.ks[[k]] = c(newdata %>% filter(assignment == k) %>% .$x %>% mean(),
                     newdata %>% filter(assignment == k) %>% .$y %>% mean())
  }
  return(list("clusters" = newdata, "means" = mean.ks))
}
```

##### (b) Hierarchical Agglomerative Clustering

```{r}
euclidVec = function(pt1, pt2){
  sqrt( (pt1[1]-pt2[1])^2 + (pt1[2]-pt2[2])^2 )
}
hier.agglo = function(data, clusters = 2){
  currentClusters = cbind(data, cluster = 1:nrow(data))
  num_cluster = length(unique(currentClusters$cluster))
  TSPmatrix = matrix(NA, nrow = nrow(data), ncol = nrow(data))
  for (i in 1:nrow(data)){
    for(j in 1:nrow(data)){
      TSPmatrix[i,j] = euclidVec(pt1 = data[i,], pt2 = data[j,])
    }
  }
  diag(TSPmatrix) = 1
  while(num_cluster > clusters){
    closestPts = which(TSPmatrix == min(TSPmatrix), arr.ind = TRUE)[1,]
    merging.clusters = currentClusters$cluster[closestPts]
    new.cluster = min(merging.clusters)
    old.cluster = max(merging.clusters)
    currentClusters$cluster[currentClusters$cluster == old.cluster] = new.cluster
    TSPmatrix[closestPts[1],closestPts[2]] = 1
    TSPmatrix[closestPts[2],closestPts[1]] = 1
    num_cluster = length(unique(currentClusters$cluster))
  }
  return(currentClusters)
}
```

##### (c) Getting the Results

```{r, cache=TRUE}
kmeans.df = k.means(data = df, clusters = 2)
agglo.df = hier.agglo(data = df, clusters = 2)
ggplot(data = kmeans.df$clusters, aes(x = x, y = y)) +
  geom_point(aes(color = assignment))
ggplot(data = agglo.df, aes(x = x, y = y)) +
  geom_point(aes(color = cluster))
```

The hierarchical model seems to perform better. This is due to the process of fitting the clusters. One takes into acount distances between individual points whereas the other is based on distance to a center that may not be a point in the data.

##### (d) Pre-Process Improvements

If we are given the EDA, as we have in figure 3, I can think of at least one thing. Bring in the third dimension. In this case, use 
$$
z = f(x,y) = -a\left((x-0.5)^2+(y-0.5)^2\right).
$$

This would raise the center points above the plane (height controlled by $a$) and make the clusters distinct when running the k-means alforithm in $\mathbb{R}^3$.